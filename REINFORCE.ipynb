{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook with the PolicyGradient content of the RL assignment (agent and REINFORCE algorithm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Louis\\anaconda3\\envs\\RL\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "import time\n",
    "import IPython\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Policy Gradient agent class\n",
    "class PolicyGradientAgent(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(PolicyGradientAgent, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the observation through the NN\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def policy(self, observation):\n",
    "        # Chose an action to do stochastically for a given state\n",
    "        observation = torch.from_numpy(np.array(observation)).unsqueeze(0).float().to(device)\n",
    "        # Compute the probabilities of the actions for this state\n",
    "        probs = self.forward(observation).cpu()\n",
    "        # To categorical\n",
    "        m = Categorical(probs)\n",
    "        # Sample an action from the probability distribution of the output\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "\n",
    "    def greedy_policy(self, observation):\n",
    "        # Chose the action with the highest probability for a given state\n",
    "        observation = torch.from_numpy(np.array(observation)).unsqueeze(0).float().to(device)\n",
    "        probs = self.forward(observation).cpu()\n",
    "        return np.argmax(probs.detach().numpy())\n",
    "\n",
    "    def save(self, path):\n",
    "        # Save the agent policy (the state dict)\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        # Load a saved policy (a state dict)\n",
    "        self.load_state_dict(torch.load(path))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE algorithm :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(agent, env, optimizer, n_training_episodes, max_t, gamma, print_every=None, disp_tqdm=True, neg_reward=0):\n",
    "    \"\"\"This function trains a Policy Gradient agent over n_trainining_episodes, with some\n",
    "        display parameter and hyperparameters. It returns the score monitoring of the\n",
    "        agent over the episodes to plot it if desired afterward.\"\"\"\n",
    "\n",
    "    rewards_deque = deque(maxlen=100)\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "\n",
    "    # Iterate over the episodes\n",
    "    if disp_tqdm:\n",
    "        iterator = tqdm(range(1, n_training_episodes + 1), desc=\"Training\")\n",
    "    else:\n",
    "        iterator = range(1, n_training_episodes + 1)\n",
    "    for i_episode in iterator:\n",
    "        # Initialise the episode history and the environment\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()[0]\n",
    "\n",
    "        # Iterate over the time steps (bounded by max_t)\n",
    "        for t in range(max_t):\n",
    "            # Take an action\n",
    "            action, log_prob = agent.policy(state)\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            reward = 1 if reward else neg_reward\n",
    "\n",
    "            # Save the log probability and the reward\n",
    "            saved_log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Save the score\n",
    "        rewards_deque.append(sum(rewards))\n",
    "        # scores.append(sum(rewards))\n",
    "        scores_deque.append(info[\"score\"])\n",
    "        scores.append(info[\"score\"])\n",
    "\n",
    "        # Calculate the return\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = returns[0] if len(returns) > 0 else 0\n",
    "            returns.appendleft(gamma * disc_return_t + rewards[t])\n",
    "\n",
    "        # standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        # Compute the loss\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if print_every is not None and i_episode % print_every == 0:\n",
    "            print(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n",
    "            print(\"Loss: {}\".format(policy_loss.item()))\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reinforce_evaluate(agent, env, optimizer, n_training_episodes, max_t, gamma, print_every=None, disp_tqdm=True, neg_reward=0):\n",
    "    \"\"\"This function is almost the same as above, but performs evaluation of the training agent\n",
    "    throughout its training loop, to monitor its improvement over time.\"\"\"\n",
    "\n",
    "    rewards_deque = deque(maxlen=100)\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    episodes = []\n",
    "\n",
    "    # Iterate over the episodes\n",
    "    if disp_tqdm:\n",
    "        iterator = tqdm(range(1, n_training_episodes + 1), desc=\"Training\")\n",
    "    else:\n",
    "        iterator = range(1, n_training_episodes + 1)\n",
    "    for i_episode in iterator:\n",
    "        # Initialise the episode history and the environment\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()[0]\n",
    "\n",
    "        # Iterate over the time steps (bounded by max_t)\n",
    "        for t in range(max_t):\n",
    "            # Take an action\n",
    "            action, log_prob = agent.policy(state)\n",
    "            state, reward, done, _, info = env.step(action)\n",
    "            reward = reward if reward else neg_reward\n",
    "\n",
    "            # Save the log probability and the reward\n",
    "            saved_log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Save the score\n",
    "        rewards_deque.append(sum(rewards))\n",
    "        # scores.append(sum(rewards))\n",
    "        scores_deque.append(info[\"score\"])\n",
    "        scores.append(info[\"score\"])\n",
    "        episodes.append(i_episode)\n",
    "\n",
    "        # Calculate the return\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = returns[0] if len(returns) > 0 else 0\n",
    "            returns.appendleft(gamma * disc_return_t + rewards[t])\n",
    "\n",
    "        # standardization of the returns is employed to make training more stable\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        # Compute the loss\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # Gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if print_every is not None and i_episode % print_every == 0:\n",
    "            print(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n",
    "            print(\"Loss: {}\".format(policy_loss.item()))\n",
    "\n",
    "\n",
    "        # Test every 100 episodes if agent converged\n",
    "        if i_episode%100 == 0:\n",
    "            # Test 10 episodes with greedy policy to see if the agent beats the game\n",
    "            greedy_score, win = evaluate_score_reinforce(env, agent, n_episodes=10, disp_tqdm=False)\n",
    "            # If agent wins 10 times, stop the learning\n",
    "            if win==10:\n",
    "                scores += [1000 for j in range(i_episode, n_training_episodes)]\n",
    "                episodes += [j for j in range(i_episode, n_training_episodes)]\n",
    "                break\n",
    "\n",
    "    return scores, episodes\n",
    "\n",
    "\n",
    "def evaluate_score_reinforce(env, agent, n_episodes=100, disp_tqdm=True):\n",
    "    \"\"\" This function enables to evaluate \n",
    "        a policy gradient agent post-training\"\"\"\n",
    "\n",
    "    scores = []\n",
    "    win = 0\n",
    "\n",
    "    if disp_tqdm:\n",
    "        iterator = tqdm(range(n_episodes))\n",
    "    else:\n",
    "        iterator = range(n_episodes)\n",
    "    \n",
    "    for _ in iterator:\n",
    "        obs = env.reset()\n",
    "        obs = obs[0]\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.greedy_policy(obs)\n",
    "            next_observation, reward, done, _, info = env.step(action)\n",
    "            obs = next_observation\n",
    "            score = info[\"score\"]\n",
    "\n",
    "            if score > 1000:\n",
    "                win += 1\n",
    "                done = True\n",
    "                score = np.nan\n",
    "\n",
    "        scores.append(score)\n",
    "    return np.nanmean(scores), win"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "s_size = 2\n",
    "h_size = 16\n",
    "\n",
    "n_training_episodes = 500\n",
    "max_t = 3000\n",
    "gamma = 0.99\n",
    "print_every = 100\n",
    "\n",
    "# Print the device\n",
    "print(\"Device: {}\".format(device))\n",
    "\n",
    "# Initialise the environment\n",
    "env = gym.make('TextFlappyBird-v0', height=15, width=20, pipe_gap=4)\n",
    "\n",
    "# Initialise the policy\n",
    "agent = PolicyGradientAgent(s_size, env.action_space.n, h_size).to(device)\n",
    "optimizer = torch.optim.Adam(agent.parameters(), lr=1e-2)\n",
    "\n",
    "# Train the policy\n",
    "scores = reinforce(agent, env, optimizer, n_training_episodes, max_t, gamma, print_every)\n",
    "\n",
    "# Save the policy\n",
    "agent.save(\"./policy_gradient_agent.pth\")\n",
    "\n",
    "print(\"Training completed\")\n",
    "\n",
    "plt.plot(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
